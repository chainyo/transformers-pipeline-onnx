{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugging Face ðŸ¤— NLP Transformers pipelines with ONNX\n",
    "\n",
    "![logo](assets/logo.png)\n",
    "\n",
    "*This project is linked to the Medium blog post: [How to use Hugging Face ðŸ¤— Transformers with ONNX in realÂ world]()*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working environment\n",
    "\n",
    "First of all, you need to install all required dependencies. It is recommended to use and isolated environment to avoid conflicts.\n",
    "\n",
    "You can use any package manager you want. I recommend [`conda`](https://conda.io/).\n",
    "\n",
    "```bash\n",
    "conda create -y -n hf-onnx python=3.8\n",
    "```\n",
    "\n",
    "The project requires Python 3.8 or higher.\n",
    "\n",
    "All required dependencies are listed in the `requirements.txt` file. To install them, run the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring colorama: markers 'platform_system == \"Windows\" and python_full_version >= \"3.6.0\" and python_version >= \"3.6\"' don't match your environment\n",
      "Ignoring pyreadline3: markers 'sys_platform == \"win32\" and python_version >= \"3.8\" and (python_version >= \"2.7\" and python_full_version < \"3.0.0\" or python_full_version >= \"3.5.0\")' don't match your environment\n",
      "Requirement already satisfied: certifi==2021.10.8 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 1)) (2021.10.8)\n",
      "Collecting charset-normalizer==2.0.12\n",
      "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
      "Requirement already satisfied: click==8.0.4 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (8.0.4)\n",
      "Requirement already satisfied: coloredlogs==15.0.1 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (15.0.1)\n",
      "Requirement already satisfied: filelock==3.6.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 16)) (3.6.0)\n",
      "Collecting flatbuffers==2.0\n",
      "  Using cached flatbuffers-2.0-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: huggingface-hub==0.4.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 22)) (0.4.0)\n",
      "Requirement already satisfied: humanfriendly==10.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 25)) (10.0)\n",
      "Requirement already satisfied: idna==3.3 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 28)) (3.3)\n",
      "Collecting joblib==1.1.0\n",
      "  Using cached joblib-1.1.0-py2.py3-none-any.whl (306 kB)\n",
      "Requirement already satisfied: numpy==1.22.2 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 34)) (1.22.2)\n",
      "Requirement already satisfied: onnx==1.11.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 54)) (1.11.0)\n",
      "Requirement already satisfied: onnxconverter-common==1.9.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 80)) (1.9.0)\n",
      "Requirement already satisfied: onnxruntime-tools==1.7.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 83)) (1.7.0)\n",
      "Collecting onnxruntime==1.10.0\n",
      "  Using cached onnxruntime-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: packaging==21.3 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 106)) (21.3)\n",
      "Requirement already satisfied: protobuf==3.19.4 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 109)) (3.19.4)\n",
      "Collecting psutil==5.9.0\n",
      "  Using cached psutil-5.9.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (283 kB)\n",
      "Requirement already satisfied: py-cpuinfo==8.0.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 169)) (8.0.0)\n",
      "Requirement already satisfied: py3nvml==0.2.7 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 171)) (0.2.7)\n",
      "Requirement already satisfied: pyparsing==3.0.7 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 174)) (3.0.7)\n",
      "Requirement already satisfied: pyyaml==6.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 180)) (6.0)\n",
      "Requirement already satisfied: regex==2022.3.2 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 214)) (2022.3.2)\n",
      "Collecting requests==2.27.1\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Requirement already satisfied: sacremoses==0.0.47 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 292)) (0.0.47)\n",
      "Requirement already satisfied: sentencepiece==0.1.96 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 294)) (0.1.96)\n",
      "Requirement already satisfied: six==1.16.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 337)) (1.16.0)\n",
      "Requirement already satisfied: tf2onnx==1.8.4 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 340)) (1.8.4)\n",
      "Requirement already satisfied: tokenizers==0.11.6 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 342)) (0.11.6)\n",
      "Requirement already satisfied: torch==1.10.2 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 381)) (1.10.2+cu113)\n",
      "Requirement already satisfied: tqdm==4.63.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 401)) (4.63.0)\n",
      "Collecting transformers==4.17.0\n",
      "  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions==4.1.1 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 407)) (4.1.1)\n",
      "Collecting urllib3==1.26.8\n",
      "  Using cached urllib3-1.26.8-py2.py3-none-any.whl (138 kB)\n",
      "Requirement already satisfied: xmltodict==0.12.0 in /home/chainyo/miniconda3/envs/onnx-hf/lib/python3.8/site-packages (from -r requirements.txt (line 413)) (0.12.0)\n",
      "Installing collected packages: flatbuffers, urllib3, psutil, onnxruntime, joblib, charset-normalizer, requests, transformers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.16.2\n",
      "    Uninstalling transformers-4.16.2:\n",
      "      Successfully uninstalled transformers-4.16.2\n",
      "Successfully installed charset-normalizer-2.0.12 flatbuffers-2.0 joblib-1.1.0 onnxruntime-1.10.0 psutil-5.9.0 requests-2.27.1 transformers-4.17.0 urllib3-1.26.8\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the model to ONNX\n",
    "\n",
    "For this example, we can use any TokenClassification model from Hugging Face's library because the task we are trying to solve is `Named Entity Recognition` (NER). \n",
    "\n",
    "I chose [`dslim/bert-base-NER`](https://huggingface.co/dslim/bert-base-NER) model because it is a `base` model which means medium computation time on CPU. Plus, BERT architecture is a good choice for NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from onnxruntime import (\n",
    "    InferenceSession, SessionOptions, GraphOptimizationLevel\n",
    ")\n",
    "from transformers import (\n",
    "    TokenClassificationPipeline, AutoTokenizer, AutoModelForTokenClassification\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = SessionOptions()\n",
    "options.graph_optimization_level = GraphOptimizationLevel.ORT_ENABLE_ALL\n",
    "session = InferenceSession(\n",
    "    \"onnx/model.onnx\", sess_options=options, providers=[\"CPUExecutionProvider\"]\n",
    ")\n",
    "session.disable_fallback()\n",
    "\n",
    "\n",
    "class OnnxTokenClassificationPipeline(TokenClassificationPipeline):\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    \n",
    "    def _forward(self, model_inputs):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \"\"\"\n",
    "        special_tokens_mask = model_inputs.pop(\"special_tokens_mask\")\n",
    "        offset_mapping = model_inputs.pop(\"offset_mapping\", None)\n",
    "        sentence = model_inputs.pop(\"sentence\")\n",
    "\n",
    "        inputs = {k: v.cpu().detach().numpy() for k, v in model_inputs.items()}\n",
    "        outputs_name = session.get_outputs()[0].name\n",
    "\n",
    "        logits = session.run(output_names=[outputs_name], input_feed=inputs)[0]\n",
    "\n",
    "        return {\n",
    "            \"logits\": torch.tensor(logits),\n",
    "            \"special_tokens_mask\": special_tokens_mask,\n",
    "            \"offset_mapping\": offset_mapping,\n",
    "            \"sentence\": sentence,\n",
    "            **model_inputs,\n",
    "        }\n",
    "\n",
    "    \n",
    "    def preprocess(self, sentence, offset_mapping=None):\n",
    "        truncation = True if self.tokenizer.model_max_length and self.tokenizer.model_max_length > 0 else False\n",
    "        model_inputs = self.tokenizer(\n",
    "            sentence,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=self.framework,\n",
    "            truncation=truncation,\n",
    "            return_special_tokens_mask=True,\n",
    "            return_offsets_mapping=self.tokenizer.is_fast,\n",
    "        )\n",
    "        if offset_mapping:\n",
    "            model_inputs[\"offset_mapping\"] = offset_mapping\n",
    "\n",
    "        model_inputs[\"sentence\"] = sentence\n",
    "\n",
    "        return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name_from_hub = \"Jean-Baptiste/roberta-large-ner-english\"\n",
    "model_name_from_hub = \"dslim/bert-base-NER\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_from_hub)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name_from_hub)\n",
    "\n",
    "ner_pipeline = OnnxTokenClassificationPipeline(\n",
    "    task=\"ner\", \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    framework=\"pt\",\n",
    "    aggregation_strategy=\"simple\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'ORG',\n",
       "  'score': 0.9978969,\n",
       "  'word': 'Apple',\n",
       "  'start': 0,\n",
       "  'end': 5},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.9981243,\n",
       "  'word': 'Steve Jobs',\n",
       "  'start': 29,\n",
       "  'end': 39},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.9741297,\n",
       "  'word': 'Steve Wozniak',\n",
       "  'start': 41,\n",
       "  'end': 54},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.99970996,\n",
       "  'word': 'Ronald Wayne',\n",
       "  'start': 59,\n",
       "  'end': 71},\n",
       " {'entity_group': 'PER',\n",
       "  'score': 0.86664414,\n",
       "  'word': 'Wozniak',\n",
       "  'start': 92,\n",
       "  'end': 99},\n",
       " {'entity_group': 'MISC',\n",
       "  'score': 0.99852806,\n",
       "  'word': 'Apple I',\n",
       "  'start': 102,\n",
       "  'end': 109}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = \"Apple was founded in 1976 by Steve Jobs, Steve Wozniak and Ronald Wayne to develop and sell Wozniak's Apple I personal computer\"\n",
    "\n",
    "ner_pipeline(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0be00ba07bf63a0c73ab9bd0f7846fd29739e80e837fef3831f52181d58986e5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('onnx-hf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
